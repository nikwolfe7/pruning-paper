\subsection{Linear Approximation Approach}
\input{diagram.tex}
We define the following network terminology here which will be used in this section and all subsequent sections  unless stated otherwise. Figure \ref{fig:comp_graph} can be used as a reference to the terminology defined here:

\begin{align}
E &= \frac{1}{2}\sum\limits_i (\Out i 0 - \Target i)^2 &
\Out i m &= \sigma(\Input i m) &
\Input i m &= \sum\limits_j {\Weight j i m}{ \Out j {m + 1}} &
\Con j i m = \Weight j i m \Out j {m+1}\label{eq:term}
\end{align}
Superscripts represent the index of the layer of the network in question, with 0 representing the output layer. $E$ is the squared-error network cost function. Note that we are dropping the $E_n$ notation used previously as the subsequent discussion is insusceptible to the data instances. $\Out i m$ is the $i$th output in layer $m$ generated by the activation function $\sigma$, which in this paper is is the standard logistic sigmoid. $\Input i m$ is the weighted sum of inputs to the $i$th neuron in the $m$th layer, and $\Con j i m$ is the contribution of the $j$th neuron in the $(m+1)$th layer to the input of the $i$th neuron in the $m$th layer. $\Weight j i m$ is the weight between the $j$th neuron in the $(m+1)$th layer and the $i$th neuron in the $m$th layer.

We can use equation \ref{eq:ts3} to get the linear error approximation of the change in error due to the $k$th neuron being turned off and represent it as $\Delta E_{k}^1$ as follows:

\begin{align}
\Delta E_{k}^1 = - o_k\cdot \left.\pdv{E}{{\Out j {m+1}}}\right|_{o_k}
\end{align}

The derivative term above is the first-order gradient which represents the change in error with respect to the output of a given neuron $o_j$ in the $(m+1)$th layer. This term can be collected during back-propagation. The derivative term above can be calculated as follows:

\begin{align}
\pdv{E}{{\Out j {m+1}}} = \sum\limits_i \pdv{E}{\Input i m}\cdot \Weight j i m
\end{align}

The full step-by-step mathematical derivation of the above equation can be found in the appendix.