\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{sietsma1988neural}
\citation{baum1989size}
\citation{chauvin1990generalization}
\citation{reed1993pruning}
\citation{segee1991fault}
\citation{mozer1989using}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{1}{section.2}}
\citation{srivastava2014dropout}
\citation{goodfellow2013maxout}
\citation{fahlman1989cascade}
\citation{balzer1991weight}
\citation{dundar1994effects}
\citation{hoehfeld1992learning}
\citation{mozer1989skeletonization}
\citation{lecun1989optimal}
\citation{hassibi1993second}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{2}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Brute Force Removal Approach}{3}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Taylor Series Representation of Error}{3}{subsection.3.2}}
\newlabel{eq:ts1}{{1}{3}{Taylor Series Representation of Error}{equation.3.1}{}}
\newlabel{eq:ts2}{{2}{3}{Taylor Series Representation of Error}{equation.3.2}{}}
\newlabel{eq:ts3}{{3}{3}{Taylor Series Representation of Error}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Linear Approximation Approach}{3}{subsection.3.3}}
\newlabel{eq:term}{{4}{3}{Linear Approximation Approach}{equation.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Quadratic Approximation Approach}{4}{subsection.3.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A computational graph of a simple feed-forward network illustrating the naming of different variables, where $\sigma (\cdot )$ is the nonlinearity, MSE is the mean-squared error cost function and $E$ is the overall loss.}}{4}{figure.1}}
\newlabel{fig:comp_graph}{{1}{4}{A computational graph of a simple feed-forward network illustrating the naming of different variables, where $\sigma (\cdot )$ is the nonlinearity, MSE is the mean-squared error cost function and $E$ is the overall loss}{figure.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The intuition behind neuron pruning decision.}}{5}{figure.2}}
\newlabel{fig:intuition}{{2}{5}{The intuition behind neuron pruning decision}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Proposed Pruning Algorithm}{5}{subsection.3.5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.1}Algorithm I: Single Overall Ranking}{6}{subsubsection.3.5.1}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Single Overall Ranking}}{6}{algocf.1}}
\newlabel{algo1}{{1}{6}{Algorithm I: Single Overall Ranking}{algocf.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.5.2}Algorithm II: Iterative Re-Ranking}{6}{subsubsection.3.5.2}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Iterative Re-Ranking}}{6}{algocf.2}}
\newlabel{algo2}{{2}{6}{Algorithm II: Iterative Re-Ranking}{algocf.2}{}}
\citation{mozer1989skeletonization}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Results}{7}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}MNIST Dataset Results}{7}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Pruning A 1-Layer Network: Single-Pass Ranking}{7}{subsubsection.4.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Degradation in squared error (left) and classification accuracy (right) after pruning a single-layer network trained on MNIST using a single-pass overall ranking procedure (\textbf  {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}}{7}{figure.3}}
\newlabel{fig:mnist-single-ranking-single-layer}{{3}{7}{Degradation in squared error (left) and classification accuracy (right) after pruning a single-layer network trained on MNIST using a single-pass overall ranking procedure (\textbf {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}{figure.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Pruning A 1-Layer Network: Re-Ranking After Each Removal}{7}{subsubsection.4.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Visualization of Error Surface \& Pruning Decisions}{7}{subsubsection.4.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.4}Pruning A 2-Layer Network: Single-Pass Ranking}{8}{subsubsection.4.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Degradation in squared error (left) and classification accuracy (right) after pruning a single-layer network trained on MNIST using an iterative re-ranking procedure (\textbf  {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}}{8}{figure.4}}
\newlabel{fig:mnist-re-ranking-single-layer}{{4}{8}{Degradation in squared error (left) and classification accuracy (right) after pruning a single-layer network trained on MNIST using an iterative re-ranking procedure (\textbf {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}{figure.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.5}Pruning A 2-Layer Network: Re-Ranking After Each Removal}{9}{subsubsection.4.1.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf  {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}}{9}{figure.5}}
\newlabel{fig:mnist-gt-single-layer}{{5}{9}{Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf  {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}}{9}{figure.6}}
\newlabel{fig:mnist-gt-single-layer}{{6}{9}{Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf  {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}}{10}{figure.7}}
\newlabel{fig:mnist-gt-single-layer}{{7}{10}{Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Degradation in squared error (left) and classification accuracy (right) after pruning a 2-layer network trained on MNIST using a single-pass overall ranking procedure (\textbf  {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}}{10}{figure.8}}
\newlabel{fig:mnist-single-ranking-double-layer}{{8}{10}{Degradation in squared error (left) and classification accuracy (right) after pruning a 2-layer network trained on MNIST using a single-pass overall ranking procedure (\textbf {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Degradation in squared error (left) and classification accuracy (right) after pruning a 2-layer network trained on MNIST using an iterative re-ranking procedure (\textbf  {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}}{10}{figure.9}}
\newlabel{fig:mnist-re-ranking-double-layer}{{9}{10}{Degradation in squared error (left) and classification accuracy (right) after pruning a 2-layer network trained on MNIST using an iterative re-ranking procedure (\textbf {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}{figure.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.6}Visualization of Error Surface \& Pruning Decisions}{11}{subsubsection.4.1.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf  {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}}{11}{figure.10}}
\newlabel{fig:mnist-gt-double-layer}{{10}{11}{Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}{figure.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf  {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}}{11}{figure.11}}
\newlabel{fig:mnist-g1-double-layer}{{11}{11}{Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf  {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}}{12}{figure.12}}
\newlabel{fig:mnist-g2-double-layer}{{12}{12}{Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}{figure.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion of Results}{13}{section.5}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusions \& Future Work}{13}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Conclusions}{13}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Future Work}{13}{subsection.6.2}}
\bibstyle{iclr2016_conference}
\bibdata{iclr2016_bib}
\bibcite{balzer1991weight}{{1}{1991}{{Balzer et~al.}}{{Balzer, Takahashi, Ohta, and Kyuma}}}
\bibcite{baum1989size}{{2}{1989}{{Baum \& Haussler}}{{Baum and Haussler}}}
\bibcite{chauvin1990generalization}{{3}{1990}{{Chauvin}}{{}}}
\bibcite{dundar1994effects}{{4}{1994}{{Dundar \& Rose}}{{Dundar and Rose}}}
\bibcite{fahlman1989cascade}{{5}{1989}{{Fahlman \& Lebiere}}{{Fahlman and Lebiere}}}
\bibcite{goodfellow2013maxout}{{6}{2013}{{Goodfellow et~al.}}{{Goodfellow, Warde-Farley, Mirza, Courville, and Bengio}}}
\bibcite{hassibi1993second}{{7}{1993}{{Hassibi \& Stork}}{{Hassibi and Stork}}}
\bibcite{hoehfeld1992learning}{{8}{1992}{{Hoehfeld \& Fahlman}}{{Hoehfeld and Fahlman}}}
\bibcite{lecun1989optimal}{{9}{1989}{{LeCun et~al.}}{{LeCun, Denker, Solla, Howard, and Jackel}}}
\bibcite{mozer1989skeletonization}{{10}{1989{a}}{{Mozer \& Smolensky}}{{Mozer and Smolensky}}}
\bibcite{mozer1989using}{{11}{1989{b}}{{Mozer \& Smolensky}}{{Mozer and Smolensky}}}
\bibcite{reed1993pruning}{{12}{1993}{{Reed}}{{}}}
\bibcite{segee1991fault}{{13}{1991}{{Segee \& Carter}}{{Segee and Carter}}}
\bibcite{sietsma1988neural}{{14}{1988}{{Sietsma \& Dow}}{{Sietsma and Dow}}}
\bibcite{srivastava2014dropout}{{15}{2014}{{Srivastava et~al.}}{{Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov}}}
