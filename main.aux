\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{sietsma1988neural}
\citation{baum1989size}
\citation{chauvin1990generalization}
\citation{reed1993pruning}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{mozer1989skeletonization}
\@writefile{toc}{\contentsline {section}{\numberline {2}Experimental Results}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}MNIST Dataset Results}{2}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Pruning A 1-Layer Network: Single-Pass Ranking}{2}{subsubsection.2.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Degradation in squared error (left) and classification accuracy (right) after pruning a single-layer network trained on MNIST using a single-pass overall ranking procedure (\textbf  {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}}{2}{figure.1}}
\newlabel{fig:mnist-single-ranking-single-layer}{{1}{2}{Degradation in squared error (left) and classification accuracy (right) after pruning a single-layer network trained on MNIST using a single-pass overall ranking procedure (\textbf {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}{figure.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Pruning A 1-Layer Network: Re-Ranking After Each Removal}{2}{subsubsection.2.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Visualization of Error Surface \& Pruning Decisions}{2}{subsubsection.2.1.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Pruning A 2-Layer Network: Single-Pass Ranking}{3}{subsubsection.2.1.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Degradation in squared error (left) and classification accuracy (right) after pruning a single-layer network trained on MNIST using an iterative re-ranking procedure (\textbf  {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}}{3}{figure.2}}
\newlabel{fig:mnist-re-ranking-single-layer}{{2}{3}{Degradation in squared error (left) and classification accuracy (right) after pruning a single-layer network trained on MNIST using an iterative re-ranking procedure (\textbf {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.5}Pruning A 2-Layer Network: Re-Ranking After Each Removal}{4}{subsubsection.2.1.5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf  {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}}{4}{figure.3}}
\newlabel{fig:mnist-gt-single-layer}{{3}{4}{Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf  {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}}{4}{figure.4}}
\newlabel{fig:mnist-gt-single-layer}{{4}{4}{Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf  {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}}{5}{figure.5}}
\newlabel{fig:mnist-gt-single-layer}{{5}{5}{Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Degradation in squared error (left) and classification accuracy (right) after pruning a 2-layer network trained on MNIST using a single-pass overall ranking procedure (\textbf  {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}}{5}{figure.6}}
\newlabel{fig:mnist-single-ranking-double-layer}{{6}{5}{Degradation in squared error (left) and classification accuracy (right) after pruning a 2-layer network trained on MNIST using a single-pass overall ranking procedure (\textbf {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Degradation in squared error (left) and classification accuracy (right) after pruning a 2-layer network trained on MNIST using an iterative re-ranking procedure (\textbf  {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}}{5}{figure.7}}
\newlabel{fig:mnist-re-ranking-double-layer}{{7}{5}{Degradation in squared error (left) and classification accuracy (right) after pruning a 2-layer network trained on MNIST using an iterative re-ranking procedure (\textbf {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}{figure.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.6}Visualization of Error Surface \& Pruning Decisions}{6}{subsubsection.2.1.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf  {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}}{6}{figure.8}}
\newlabel{fig:mnist-gt-double-layer}{{8}{6}{Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}{figure.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf  {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}}{6}{figure.9}}
\newlabel{fig:mnist-g1-double-layer}{{9}{6}{Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf  {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}}{7}{figure.10}}
\newlabel{fig:mnist-g2-double-layer}{{10}{7}{Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal; (\textbf {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}{figure.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Conclusions \& Future Work}{8}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Conclusions}{8}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Future Work}{8}{subsection.3.2}}
\bibstyle{iclr2016_conference}
\bibdata{iclr2016_bib}
\bibcite{baum1989size}{{1}{1989}{{Baum \& Haussler}}{{Baum and Haussler}}}
\bibcite{chauvin1990generalization}{{2}{1990}{{Chauvin}}{{}}}
\bibcite{mozer1989skeletonization}{{3}{1989}{{Mozer \& Smolensky}}{{Mozer and Smolensky}}}
\bibcite{reed1993pruning}{{4}{1993}{{Reed}}{{}}}
\bibcite{sietsma1988neural}{{5}{1988}{{Sietsma \& Dow}}{{Sietsma and Dow}}}
