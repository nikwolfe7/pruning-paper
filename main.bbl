\begin{thebibliography}{20}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Balzer et~al.(1991)Balzer, Takahashi, Ohta, and
  Kyuma]{balzer1991weight}
Balzer, Wolfgang, Takahashi, Masanobu, Ohta, Jun, and Kyuma, Kazuo.
\newblock Weight quantization in boltzmann machines.
\newblock \emph{Neural Networks}, 4\penalty0 (3):\penalty0 405--409, 1991.

\bibitem[Baum \& Haussler(1989)Baum and Haussler]{baum1989size}
Baum, Eric~B and Haussler, David.
\newblock What size net gives valid generalization?
\newblock \emph{Neural computation}, 1\penalty0 (1):\penalty0 151--160, 1989.

\bibitem[Chauvin(1990)]{chauvin1990generalization}
Chauvin, Yves.
\newblock Generalization performance of overtrained back-propagation networks.
\newblock In \emph{Neural Networks}, pp.\  45--55. Springer, 1990.

\bibitem[Dundar \& Rose(1994)Dundar and Rose]{dundar1994effects}
Dundar, Gunhan and Rose, Kenneth.
\newblock The effects of quantization on multilayer neural networks.
\newblock \emph{IEEE transactions on neural networks/a publication of the IEEE
  Neural Networks Council}, 6\penalty0 (6):\penalty0 1446--1451, 1994.

\bibitem[Fahlman \& Lebiere(1989)Fahlman and Lebiere]{fahlman1989cascade}
Fahlman, Scott~E and Lebiere, Christian.
\newblock The cascade-correlation learning architecture.
\newblock 1989.

\bibitem[Goodfellow et~al.(2013)Goodfellow, Warde-Farley, Mirza, Courville, and
  Bengio]{goodfellow2013maxout}
Goodfellow, Ian~J, Warde-Farley, David, Mirza, Mehdi, Courville, Aaron, and
  Bengio, Yoshua.
\newblock Maxout networks.
\newblock \emph{arXiv preprint arXiv:1302.4389}, 2013.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{deepcompression2016}
Han, Song, Mao, Huizi, and Dally, William~J.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149v5}, 2016.

\bibitem[Hassibi \& Stork(1993)Hassibi and Stork]{hassibi1993second}
Hassibi, Babak and Stork, David~G.
\newblock \emph{Second order derivatives for network pruning: Optimal brain
  surgeon}.
\newblock Morgan Kaufmann, 1993.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{darkknowledge2015}
Hinton, Geoffrey, Vinyals, Oriol, and Dean, Jeff.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hoehfeld \& Fahlman(1992)Hoehfeld and Fahlman]{hoehfeld1992learning}
Hoehfeld, Markus and Fahlman, Scott~E.
\newblock Learning with limited numerical precision using the
  cascade-correlation algorithm.
\newblock \emph{IEEE Transactions on Neural Networks}, 3\penalty0 (4):\penalty0
  602--611, 1992.

\bibitem[LeCun \& Cortes(2010)LeCun and
  Cortes]{lecun-mnisthandwrittendigit-2010}
LeCun, Yann and Cortes, Corinna.
\newblock {MNIST} handwritten digit database.
\newblock 2010.

\bibitem[LeCun et~al.(1989)LeCun, Denker, Solla, Howard, and
  Jackel]{lecun1989optimal}
LeCun, Yann, Denker, John~S, Solla, Sara~A, Howard, Richard~E, and Jackel,
  Lawrence~D.
\newblock Optimal brain damage.
\newblock In \emph{NIPs}, volume~89, 1989.

\bibitem[Mozer \& Smolensky(1989{\natexlab{a}})Mozer and
  Smolensky]{mozer1989skeletonization}
Mozer, Michael~C and Smolensky, Paul.
\newblock Skeletonization: A technique for trimming the fat from a network via
  relevance assessment.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  107--115, 1989{\natexlab{a}}.

\bibitem[Mozer \& Smolensky(1989{\natexlab{b}})Mozer and
  Smolensky]{mozer1989using}
Mozer, Michael~C and Smolensky, Paul.
\newblock Using relevance to reduce network size automatically.
\newblock \emph{Connection Science}, 1\penalty0 (1):\penalty0 3--16,
  1989{\natexlab{b}}.

\bibitem[Oland \& Raj(2015)Oland and Raj]{Anders2016quant}
Oland, Anders and Raj, Bhiksha.
\newblock Reducing communication overhead in distributed learning by an order
  of magnitude (almost).
\newblock In \emph{2015 {IEEE} International Conference on Acoustics, Speech
  and Signal Processing, {ICASSP} 2015, South Brisbane, Queensland, Australia,
  April 19-24, 2015}, pp.\  2219--2223, 2015.

\bibitem[Prabhavalkar et~al.(2016)Prabhavalkar, Alsharif, Bruguier, and
  McGraw]{prabhavalkar2016svd}
Prabhavalkar, Rohit, Alsharif, Ouais, Bruguier, Antoine, and McGraw, Ian.
\newblock On the compression of recurrent neural networks with an application
  to {LVCSR} acoustic modeling for embedded speech recognition.
\newblock \emph{arXiv preprint arXiv:1603.08042v2}, 2016.

\bibitem[Reed(1993)]{reed1993pruning}
Reed, Russell.
\newblock Pruning algorithms-a survey.
\newblock \emph{Neural Networks, IEEE Transactions on}, 4\penalty0
  (5):\penalty0 740--747, 1993.

\bibitem[Segee \& Carter(1991)Segee and Carter]{segee1991fault}
Segee, Bruce~E and Carter, Michael~J.
\newblock Fault tolerance of pruned multilayer networks.
\newblock In \emph{Neural Networks, 1991., IJCNN-91-Seattle International Joint
  Conference on}, volume~2, pp.\  447--452. IEEE, 1991.

\bibitem[Sietsma \& Dow(1988)Sietsma and Dow]{sietsma1988neural}
Sietsma, Jocelyn and Dow, Robert~JF.
\newblock Neural net pruning-why and how.
\newblock In \emph{Neural Networks, 1988., IEEE International Conference on},
  pp.\  325--333. IEEE, 1988.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, Nitish, Hinton, Geoffrey, Krizhevsky, Alex, Sutskever, Ilya, and
  Salakhutdinov, Ruslan.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\end{thebibliography}
