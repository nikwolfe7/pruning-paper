\begin{thebibliography}{5}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baum \& Haussler(1989)Baum and Haussler]{baum1989size}
Baum, Eric~B and Haussler, David.
\newblock What size net gives valid generalization?
\newblock \emph{Neural computation}, 1\penalty0 (1):\penalty0 151--160, 1989.

\bibitem[Chauvin(1990)]{chauvin1990generalization}
Chauvin, Yves.
\newblock Generalization performance of overtrained back-propagation networks.
\newblock In \emph{Neural Networks}, pp.\  45--55. Springer, 1990.

\bibitem[Mozer \& Smolensky(1989)Mozer and Smolensky]{mozer1989skeletonization}
Mozer, Michael~C and Smolensky, Paul.
\newblock Skeletonization: A technique for trimming the fat from a network via
  relevance assessment.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  107--115, 1989.

\bibitem[Reed(1993)]{reed1993pruning}
Reed, Russell.
\newblock Pruning algorithms-a survey.
\newblock \emph{Neural Networks, IEEE Transactions on}, 4\penalty0
  (5):\penalty0 740--747, 1993.

\bibitem[Sietsma \& Dow(1988)Sietsma and Dow]{sietsma1988neural}
Sietsma, Jocelyn and Dow, Robert~JF.
\newblock Neural net pruning-why and how.
\newblock In \emph{Neural Networks, 1988., IEEE International Conference on},
  pp.\  325--333. IEEE, 1988.

\end{thebibliography}
