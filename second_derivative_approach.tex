\subsection{Quadratic Approximation Approach}

As seen in equation \ref{eq:ts3}, $\Delta E_{n,k}$ which can now be represented as $\Delta E_{k}^2$ is the quadratic approximation of the change in error due to the $k$th neuron being turned off. The quadratic term in equation \ref{eq:ts3} requires some discussion which we provide here. A more detailed and step-by-step mathematical derivation can be found in the appendix.

Let us reproduce equation \ref{eq:ts3} in our new terminology here: 
\begin{align}
\Delta E_{k}^2 = - o_k\cdot \left.\pdv{E}{{\Out j {m+1}}}\right|_{o_k} + 0.5\cdot o_k^2\cdot \left.\pdv[2]{E}{{\Out j {m+1}}}\right|_{o_k}\
\end{align}

The second term here involves the second-order gradient which represents the second-order change in error with respect to the output of a given neuron $o_j$ in the $(m+1)$th layer. This term can be generated by performing back-propagation using second derivatives. A full derivation of the second derivative back-propagation can be found in the appendix. We will quote some results from the derivation here. The second-order derivative term can be represented as:

\begin{align}
\pdv[2]{E}{{\Out j {m+1}}} &= \sum_i
\pdv[2]{E}{{\Con j i m}} \left({\Weight j i m}\right)^2
\end{align} 

Here,$\Con j i m$ is one of the component terms of $\Input i m$, as follows from the equations in \ref{eq:term}. Hence, it can be easily proved that (full proof in appendix):
\begin{align}
\pdv[2]{E}{{\Con j i m}} = \pdv[2]{E}{{\Input i m}}
\end{align}

Now, the value of $\Input i m$ can be easily calculated through the steps of the second-order back-propagation using Chain Rule. The full derivation can again, be found in the appendix.
\begin{align}
\pdv[2]{E}{{\Input i m}}=\pdv[2]{E}{{\Out i m}} \left(\sigma^{\prime}\left({\Input i m}\right)\right)^2
+
\pdv{E}{{\Out i m}}\sigma^{\prime\prime}\left(\Input i m\right)
\end{align}
