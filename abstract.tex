\begin{abstract}
We propose and investigate several methods for pruning artificial neural networks to operate in constrained memory environments such as mobile or embedded devices. Unlike traditional methods that compress networks by pruning their weights, we focus on pruning entire neurons, thereby eliminating all the incoming and outgoing weights from a pruned node resulting in more memory saved. All the proposed methods prune neurons based off a "ranked list" of the neurons' contribution to overall network performance. Each method proposed uses a different way of creating this ranked list. We evaluate these methods and look into their strengths and limitations. We also evaluate 2 different algorithms to perform pruning on these created ranked lists and compare their performance. In doing so, we also get to look into the correlation between neurons in a trained network and their learning representations, using which we try to answer an age old question: are all the neurons in a network the same? We argue that within this answer lies the perfect method of pruning that allows for the optimal trade-off in network size versus accuracy in order to operate within the memory constraints of a particular device or application environment. 
\end{abstract}