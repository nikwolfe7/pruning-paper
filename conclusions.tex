\section{Conclusions}
%\subsection{Conclusions}
Pruning neurons (instead of pruning individual weights) in a pre-trained neural network without seeing a major loss in performance is not only possible but also enables compressing networks to 40-80\% of their original size, which is of great importance in constrained memory environments like embedded devices. This fact is established through the experiments using the brute force criterion, which if made computationally viable or approximated more efficiently than the Taylor Series based methods discussed in this paper, can prove to be a useful compression tool.

The experiments on the visualization of error surfaces and pruning decisions concretely establish the fact that not all neurons in a network contribute to its performance in the same way. This confirms the idea put forth by \cite{mozer1989using} that learning representation is not distributed uniformly across neurons. Neural networks use a few neurons to learn the function approximation, and the remaining neurons cooperate to cancel out each other's effects. This is also a strong indication of the fact that once training is done, bigger networks do not hold an advantage over smaller ones, which is similar to the idea put forth by \cite{darkknowledge2015} in their work on ensemble learning techniques.


\section{Future Work}

Apart from some possible future experiments listed towards the end of the Experimental Results section, this paper raises some other interesting questions that can be looked into in the future.

An obvious next step is to try out these experiments with deeper networks and perhaps using more error-change approximation criteria than only Taylor Series. It will also be nice to find out how this technique performs on other popular and real world datasets.

The brute force method presents a great case for the reduction of a neural network's memory footprint. The only downside to this method is its computational intensity, but since for a given problem pruning will only be a one-time process, perhaps looking into speedups of the brute force method through parallelization will be a nice idea.

It might also be interesting to see if the Cascade Correlation architecture arrives at the same number of final neurons as the pruning technique discussed in this paper.
