\section{Conclusions \& Future Work}

\subsection{Conclusions}
2nd-Order Method Consistently better than 1st-Order, but still pretty bad
Both Methods fail miserably beyond the 1st layer in terms of classification accuracy
For the 2-layer networks the 1st/2nd order methods do not appear to have any apparent preference for the 1st or 2nd layer in pruning decisions. 
Brute force method pulls primarily from the outer layer at the beginning -- clearly the 1st and 2nd order methods will suffer from their difference in this regard
Performance is worse if initial network is not trained to near-perfect accuracy, but pruning in this case typically improves MSE (not accuracy) for the first 5-10% of neurons removed
Brute force method is surprisingly good and indicates the degree to which extra parameters are a waste
Sum of squared errors is a decent proxy for classification accuracy, but classification accuracy is a clearer signal of performance in such problems
If Brute Force method were made comutationally viable, could be used to prune 40-80% of neurons in almost all cases with NO re-training
This empirically shows that the learning representation is not evenly distributed over neurons, indicating NO advantage to larger networks if unnecessary
Confirms Mozer \& Smolensky's conclusion in 1989: Learning representation NOT distributed!
Neurons clearly group together to cancel out each other's effects (see spikes in classification accuracy drop-off graphs), which means neural networks use a few neurons to learn the function approximation, and the rest cooperate to cancel out each other's effects!

* second derivative method works better
* still not very good
* retraining is a good idea
* speedup of brute force method
* which layer gets pruned first

\subsection{Future Work}
Future experiments (NOT for this paper):
Try with deeper networks (3/4/5 layers) to see if the performance degrades as we should expect
Speed up brute force method through parallelization?
How do results improve when we re-train after pruning 1-n neurons in a row? (expectation: smooooooooth curve for brute force method)
Examine whether Cascade Correlation arrives at the same minimal number of neurons
Use cross entropy to make pruning decision for classification problems
Try a regression task on a real-world dataset, e.g. housing prices, stock market, etc. 
