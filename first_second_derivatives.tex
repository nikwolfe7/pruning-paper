\pagebreak
\section{Appendix A: Second Derivative Back-Propagation}
Name and network definitions:\\
\begin{align}
E &= \frac{1}{2}\sum\limits_i (\Out i 0 - \Target i)^2 &
\Out i m &= \sigma(\Input i m) &
\Input i m &= \sum\limits_j {\Weight j i m}{ \Out j {m + 1}} &
\Con j i m = \Weight j i m \Out j {m+1}
\end{align}
Superscripts represent the index of the layer of the network in question, with 0 representing the output layer. $E$ is the squared-error network cost function. $\Out i m$ is the $i$th output in layer $m$ generated by the activation function $\sigma$, which in this paper is is the standard logistic sigmoid. $\Input i m$ is the weighted sum of inputs to the $i$th neuron in the $m$th layer, and $\Con j i m$ is the contribution of the $j$th neuron in the $m+1$ layer to the input of the $i$th neuron in the $m$th layer. 
\subsection{First and Second Derivatives} 
The first and second derivatives of the cost function with respect to the outputs:
\begin{align}
\pdv{E}{\Out i 0} &= \Out i 0 - \Target i \label{cost_func_derivative}
\end{align}
\begin{align}
\pdv[2]{E}{{\Out i 0}} &= 1\label{cost_func_2nd_derivative}
\end{align}
The first and second derivatives of the sigmoid function in forms depending only on the output:
\begin{align}
\sigma^{\prime}(x) &= \sigma(x)\left(1 - \sigma(x)\right)\label{sigmoid_derivative} 
\\
\sigma^{\prime\prime}(x) &= \sigma^{\prime}(x)\left(1 - 2\sigma(x)\right) \label{sigmoid_2nd_derivative}
\end{align}
The second derivative of the sigmoid is easily derived from the first derivative:
\begin{align}
\sigma^{\prime}(x) &= \sigma(x)\left(1 - \sigma(x)\right)
\\
\sigma^{\prime\prime}(x) &= \dv{}{x}
\underbrace{\sigma(x)}_{f(x)}
\underbrace{\left(1 - \sigma(x)\right)}_{g(x)}
\\
\sigma^{\prime\prime}(x) &= f^{\prime}(x)g(x) + f(x)g^{\prime}(x)
\\
\sigma^{\prime\prime}(x) &= \sigma^{\prime}(x)(1-\sigma(x)) - \sigma(x)\sigma^{\prime}(x)
\\
\sigma^{\prime\prime}(x) &= \sigma^{\prime}(x) - 2\sigma(x)\sigma^{\prime}(x)
\\
\sigma^{\prime\prime}(x) &= \sigma^{\prime}(x)(1 - 2\sigma(x))
\end{align}

And for future convenience: 
\begin{align}
\dv{\Out i m}{\Input i m} &= 
\dv{}{\Input i m}\left({\Out i m} = \sigma(\Input i m)\right) 
\\
&= \left(\Out i m\right)\left(1 - \Out i m\right)
\\
&= \sigma^{\prime}\left(\Input i m\right)
\\
\dv[2]{{\Out i m}}{{\Input i m}} &=
\dv{}{{\Input i m}}\left(\dv{\Out i m}{\Input i m} = \left(\Out i m\right)\left(1 - \Out i m\right)\right)
\\
&= \left(\Out i m\left(1 - \Out i m\right)\right)\left(1 - 2\Out i m\right)
\\
&= \sigma^{\prime\prime}\left(\Input i m\right)
\end{align}

Derivative of the error with respect to the $i$th neuron's input $\Input i 0$ in the output layer:
\begin{align}
\pdv{E}{\Input i 0} &= \pdv{E}{\Out i 0} \pdv{\Out i 0}{\Input i 0} 
\\
&= \underbrace{\left(\Out i 0 - \Target i\right)}_{\text{from} \ (\ref{cost_func_derivative})} \underbrace{\sigma\left(\Input i 0\right)\left(1 - \sigma\left(\Input i 0\right)\right)}
_{\text{from} \ (\ref{sigmoid_derivative})}
\\
&= \left(\Out i 0 - \Target i\right)\left(\Out i 0 \left(1 - \Out i 0\right)\right)
\\
&= \left(\Out i 0 - \Target i\right)\sigma^{\prime}\left(\Input i 0\right)\label{dedx}
\end{align}

Second derivative of the error with respect to the $i$th neuron's input $\Input i 0$ in the output layer:
\begin{align}
\pdv[2]{E}{{\Input i 0}} &= \pdv{}{\Input i 0}
\left(\pdv{E}{\Out i 0}\pdv{\Out i 0}{\Input i 0}\right) 
\\
&= \pdv{E}{\Input i 0}{\Out i 0}
\pdv{\Out i 0}{\Input i 0} + \pdv{E}{\Out i 0}\pdv[2]{\Out i 0}{{\Input i 0}}
\\
&= \pdv{E}{\Input i 0}{\Out i 0}
\underbrace{\left(\Out i 0 \left(1 - \Out i 0\right)\right)}_{{\text{from} \ (\ref{sigmoid_derivative})}} + \underbrace{\left(\Out i 0 - \Target i\right)}_{\text{from} \ (\ref{cost_func_derivative})}\underbrace{\left(\Out i 0 \left(1 - \Out i 0\right)\right)\left(1 - 2\Out i 0\right) }_{\text{from}\ (\ref{sigmoid_2nd_derivative})}
\\
\left(\pdv{E}{\Input i 0}{\Out i 0}\right)
&= \pdv{}{\Input i 0}\pdv{E}{\Out i 0} = \pdv{}{\Input i 0}\underbrace{\left(\Out i 0 - \Target i\right)}_{\text{from} \ (\ref{cost_func_derivative})} = \pdv{\Out i 0}{\Input i 0} = \underbrace{\left(\Out i 0 \left(1 - \Out i 0\right)\right)}_{{\text{from} \ (\ref{sigmoid_derivative})}} 
\\
\pdv[2]{E}{{\Input i 0}}&= \left(\Out i 0 \left(1 - \Out i 0\right)\right)^2 + \left(\Out i 0 - \Target i\right)\left(\Out i 0 \left(1 - \Out i 0\right)\right)\left(1 - 2\Out i 0\right) 
\\
&= \left(\sigma^{\prime}\left(\Input i 0\right)\right)^2 + \left(\Out i 0 - \Target i\right)\sigma^{\prime\prime}\left(\Input i 0\right)\label{d2edx2}
\end{align}

First derivative of the error with respect to a single input contribution $\Con j i 0$ from neuron $j$ to neuron $i$ with weight $\Weight j i 0$ in the output layer:
\begin{align}
\pdv{E}{\Con j i 0} &= 
\pdv{E}{\Out i 0}
\pdv{\Out i 0}{\Input i 0}
\pdv{\Input i 0}{\Con j i 0}
\\
&= \underbrace{\left(\Out i 0 - \Target i \right)}_{\text{from} \ (\ref{cost_func_derivative})} \underbrace{\left(\Out i 0 \left(1 - \Out i 0\right) \right)}_{\text{from} \ (\ref{sigmoid_derivative})} \pdv{\Input i 0}{\Con j i 0} 
\\
\left( \pdv{\Input i m}{\Con j i m}\right) &= \pdv{}{\Con j i m}\left(\Input i m = \sum_j\Weight j i m\Out j {m+1} \right) = \pdv{}{\Con j i m} \left(\Con j i m + k \right) = 1\label{dxdc} 
\\
\pdv{E}{\Con j i 0}&= \left(\Out i 0 - \Target i \right) \left(\Out i 0 \left(1 - \Out i 0\right) \right)
\\
&= \underbrace{\left(\Out i 0 - \Target i \right) \sigma^{\prime}\left(\Input i 0\right)\label{dedc}}
_{\text{from} \ (\ref{dedx})} 
\\
\pdv{E}{\Con j i 0} &= \pdv{E}{\Input i 0}
\end{align}

Second derivative of the error with respect to a single input contribution $\Con j i 0$:
\begin{align}
\pdv[2]{E}{{\Con j i 0}} &=
\pdv{}{\Con j i 0} 
\left(\pdv{E}{\Con j i 0} = 
\underbrace{\left(\Out i 0 - \Target i \right) \sigma^{\prime}\left(\Input i 0\right)}
_{\text{from} \ (\ref{dedc})}
\right)
\\
&=\pdv{}{\Con j i 0}\left(\sigma\left(\Input i 0\right) - \Target i \right) \sigma^{\prime}\left(\Input i 0\right)
\\
&=\pdv{}{\Con j i 0}\left(\sigma\left(\sum\limits_j {\Weight j i m}{ \Out j {m + 1}}\right) - \Target i \right) \sigma^{\prime}\left(\sum\limits_j {\Weight j i m}{ \Out j {m + 1}}\right)
\\
&=\pdv{}{\Con j i 0}\left(\sigma\left(\sum\limits_j {\Con j i 0}\right) - \Target i \right) \sigma^{\prime}\left(\sum\limits_j {\Con j i 0}\right)
\\
&=\pdv{}{\Con j i 0}
\underbrace{\left(\sigma\left({\Con j i 0} + k\right) - \Target i \right)}
_{f\left(\Con j i 0\right)}
\underbrace{\sigma^{\prime}\left({\Con j i 0} + k\right)}
_{g\left(\Con j i 0\right)}
\end{align}

We now make use of the abbreviations $f$ and $g$:
\begin{align}
&=f^{\prime}\left(\Con j i 0\right)g\left(\Con j i 0\right) + f\left(\Con j i 0\right)g^{\prime}\left(\Con j i 0\right)
\\
&=\sigma^{\prime}\left({\Con j i 0} + k\right)\sigma^{\prime}\left({\Con j i 0} + k\right) + 
\left(\sigma\left({\Con j i 0} + k\right) - \Target i \right)\sigma^{\prime\prime}\left({\Con j i 0} + k\right)
\\
&=\sigma^{\prime}\left({\Con j i 0} + k\right)^2 + 
\left(\Out i 0 - \Target i \right)\sigma^{\prime\prime}\left({\Con j i 0} + k\right)
\\
&\left(\Con j i 0 + k = \sum_j{\Con j i 0} = \sum\limits_j {\Weight j i m}{ \Out j {m + 1}} = \Input i 0 \right)
\\
\pdv[2]{E}{{\Con j i 0}}&=
\underbrace{\left(\sigma^{\prime}\left(\Input i 0\right)\right)^2 + 
\left(\Out i 0 - \Target i \right)\sigma^{\prime\prime}\left(\Input i 0\right)}
_{\text{from} \ (\ref{d2edx2})}
\\
\pdv[2]{E}{{\Con j i 0}} &= \pdv[2]{E}{{\Input i 0}}
\end{align}


\subsubsection{Summary Of Output Layer Derivatives}
\begin{align}
&\pdv{E}{\Out i 0} = \Out i 0 - \Target i 
&
\pdv[2]{E}{{\Out i 0}} = 1
\end{align}
\begin{align}
&\pdv{E}{\Input i 0} = \left(\Out i 0 - \Target i\right)\sigma^{\prime}\left(\Input i 0\right)
& 
\pdv[2]{E}{{\Input i 0}} = \left(\sigma^{\prime}\left(\Input i 0\right)\right)^2 + \left(\Out i 0 - \Target i\right)\sigma^{\prime\prime}\left(\Input i 0\right)
\end{align}
\begin{align}
&\pdv{E}{{\Con j i 0}} = \pdv{E}{{\Input i 0}}
&
\pdv[2]{E}{{\Con j i 0}} = \pdv[2]{E}{{\Input i 0}}
\end{align}


\subsubsection{Hidden Layer Derivatives}
The first derivative of the error with respect to a neuron with output $\Out j 1$ in the first hidden layer, summing over all partial derivative contributions from the output layer:
\begin{align}
\pdv{E}{\Out j 1} &= 
\sum_i
\pdv{E}{\Out i 0}
\pdv{\Out i 0}{\Input i 0}
\pdv{\Input i 0}{\Con j i 0}
\pdv{\Con j i 0}{\Out j 1}
= 
\sum_i
\underbrace{\left(\Out i 0 - \Target i\right)\sigma^{\prime}\left(\Input i 0\right)}
_{\text{from} \ (\ref{dedx})}
\Weight j i 0
\\
&\pdv{{\Con j i m}}{{\Out j {m+1}}} = \pdv{}{{\Out j {m+1}}}\left(\Con j i m = \Weight j i m\Out j {m+1}\right) = \Weight j i m\label{dcdo}
\\
\pdv{E}{\Out j 1} &= \sum_i\pdv{E}{\Input i 0}\Weight j i 0
\end{align}
Note that this equation does not depend on the specific form of $\pdv{E}{\Input i 0}$, whether it involves a sigmoid or any other activation function. We can therefore replace the specific indexes with general ones, and use this equation in the future.
\begin{align}
\pdv{E}{\Out j {m+1}} &= \sum_i\pdv{E}{\Input i m}\Weight j i m\label{dedo_general}
\end{align}
The second derivative of the error with respect to a neuron with output $\Out j 1$ in the first hidden layer:
\begin{align}
\pdv[2]{E}{{\Out j 1}} &= 
\pdv{}{\Out j 1}
\pdv{E}{\Out j 1}
\\
&= \pdv{}{\Out j 1}
\sum_i\pdv{E}{\Input i 0}\Weight j i 0
\\
&= \pdv{}{\Out j 1}
\sum_i
\left(\Out i 0 - \Target i\right)\sigma^{\prime}\left(\Input i 0\right)\Weight j i 0
\end{align}

If we now make use of the fact, that 
${\Out i 0} = \sigma\left({\Input i 0}\right) = \sigma\left(\sum_j\left({\Weight j i 0}{\Out j 1}\right)\right)$, we can evaluate the expression further.

\begin{align}
\pdv[2]{E}{{\Out j 1}}
&= \pdv{}{\Out j 1}
\sum_i
\underbrace{\left(\sigma\left(\sum_j{\Weight j i 0}{\Out j 1}\right) - \Target i\right)}
_{f\left(\Out j 1\right)}
\underbrace{\sigma^{\prime}\left(\sum_j{\Weight j i 0}{\Out j 1}\right)\Weight j i 0}
_{g\left(\Out j 1\right)}
\\
&=\sum_i\left(f^{\prime}\left(\Out j 1\right)g\left(\Out j 1\right) + f\left(\Out j 1\right)g^{\prime}\left(\Out j 1\right)\right)
\\
&=\sum_i
\sigma^{\prime}\left(\sum_j{\Weight j i 0}{\Out j 1}\right)\Weight j i 0 \
\sigma^{\prime}\left(\sum_j{\Weight j i 0}{\Out j 1}\right)\Weight j i 0
+ \ldots \\
&\sum_i
\left(\sigma\left(\sum_j{\Weight j i 0}{\Out j 1}\right) - \Target i\right)
\sigma^{\prime\prime}\left(\sum_j{\Weight j i 0}{\Out j 1}\right)\left(\Weight j i 0\right)^2
\\
&=
\sum_i\left(
\left(\sigma^{\prime}\left(\Input i 0\right)\right)^2\left({\Weight j i 0}\right)^2
+ 
\left(\Out i 0 - \Target i\right)
\sigma^{\prime\prime}\left(\Input i 0\right)\left({\Weight j i 0}\right)^2
\right)
\\
&=\sum_i
\underbrace{\left(\left(\sigma^{\prime}\left(\Input i 0\right)\right)^2
+ 
\left(\Out i 0 - \Target i\right)
\sigma^{\prime\prime}\left(\Input i 0\right)\right)}
_{\text{from} \ (\ref{d2edx2})}
\left({\Weight j i 0}\right)^2
\end{align}

Summing up, we obtain the more general expression:
\begin{align}
\pdv[2]{E}{{\Out j 1}} &= 
\sum_i\pdv[2]{E}{{\Input i 0}} \left({\Weight j i 0}\right)^2\label{d2edo2}
\end{align}
Note that the equation in (\ref{d2edo2}) does not depend on the form of $\pdv[2]{E}{{\Input x 0}}$, which means we can replace the specific indexes with general ones:
\begin{align}
\pdv[2]{E}{{\Out j {m+1}}} &= \sum_i
\pdv[2]{E}{{\Input i m}} \left({\Weight j i m}\right)^2\label{de2do2_general}
\end{align} 
At this point we are beginning to see the recursion in the form of the 2nd derivative terms which can be thought of analogously to the first derivative recursion which is central to the back-propagation algorithm. The formulation above which makes specific reference to layer indexes also works in the general case.
\\ 
Consider the $i$th neuron in any layer $m$ with output $\Out i m$ and input $\Input i m$. The first and second derivatives of the error $E$ with respect to this neuron's \textit{input} are: 
\begin{align}
\pdv{E}{\Input i m} &= 
\pdv{E}{\Out i m}
\pdv{\Out i m}{\Input i m}\label{dedx_general}
\end{align}
\begin{align}
\pdv[2]{E}{{\Input i m}} &= 
\pdv{}{{\Input i m}}
\pdv{E}{{\Input i m}} 
\\
&= \pdv{}{\Input i m}
\left(
\pdv{E}{\Out i m}
\pdv{\Out i m}{\Input i m}
\right)
\\
&= \pdv{E}{\Input i m}{\Out i m}
\pdv{\Out i m}{\Input i m}
+
\pdv{E}{\Out i m}\pdv[2]{{\Out i m}}{{\Input i m}}
\\
&=\pdv{}{{\Out i m}}
\left(\pdv{E}{\Input i m} = \pdv{E}{{\Out i m}}\pdv{{\Out i m}}{{\Input i m}}\right)
\pdv{{\Out i m}}{{\Input i m}}
+
\pdv{E}{\Out i m}\sigma^{\prime\prime}\left(\Input i m\right)
\\
&=\pdv[2]{E}{{\Out i m}}
\left
(\pdv{{\Out i m}}{{\Input i m}}
\pdv{{\Out i m}}{{\Input i m}}
\right)
+
\pdv{E}{\Out i m}\sigma^{\prime\prime}\left(\Input i m\right)
\\
\pdv[2]{E}{{\Input i m}} &= 
\pdv[2]{E}{{\Out i m}} \left(\sigma^{\prime}\left({\Input i m}\right)\right)^2
+
\pdv{E}{{\Out i m}}\sigma^{\prime\prime}\left(\Input i m\right)
\end{align}
Note the form of this equation is the general form of what was derived for the output layer in (\ref{d2edx2}). Both of the above first and second terms are easily computable and can be stored as we propagate back from the output of the network to the input. With respect to the output layer, the first and second derivative terms have already been derived above. In the case of the $m + 1$ hidden layer during back propagation, there is a summation of terms calculated in the $m$th layer. For the first derivative, we have this from (\ref{dedo_general}).
\begin{align}
\pdv{E}{\Out j {m+1}} &= \sum_i\pdv{E}{\Input i m}\Weight j i m
\end{align}
And the second derivative for the $j$th neuron in the $m+1$ layer:
\begin{align}
\pdv[2]{E}{{\Input j {m+1}}} &= 
\pdv[2]{E}{{\Out j {m+1}}}
\left(\sigma^{\prime}\left({\Input j {m+1}}\right)\right)^2
+
\pdv{E}{{\Out j {m+1}}}\sigma^{\prime\prime}\left(\Input j {m+1}\right)
\end{align}
We can replace both derivative terms with the forms which depend on the previous layer:
\begin{align}
\pdv[2]{E}{{\Input j {m+1}}} &= 
\underbrace{\sum_i\pdv[2]{E}{{\Input i 0}} \left({\Weight j i 0}\right)^2}
_{\text{from} \ (\ref{de2do2_general})}
\left(\sigma^{\prime}\left({\Input j {m+1}}\right)\right)^2
+
\underbrace{\sum_i\pdv{E}{\Input i m}\Weight j i m}
_{\text{from} \ (\ref{dedo_general})}
\sigma^{\prime\prime}\left(\Input j {m+1}\right)
\end{align}
And this horrible mouthful of an equation gives you a general form for any neuron in the $j$th position of the $m+1$ layer. Taking very careful note of the indexes, this can be more or less translated painlessly to code. You are welcome, world.

\subsubsection{Summary Of Hidden Layer Derivatives}
\begin{align}
\pdv{E}{\Out j {m+1}} &= \sum_i\pdv{E}{\Input i m}\Weight j i m &
\pdv[2]{E}{{\Out j {m+1}}} &= 
\sum_i\pdv[2]{E}{{\Input i m}} \left({\Weight j i m}\right)^2
\end{align}
\begin{align}
\pdv{E}{\Input i m} &= 
\pdv{E}{\Out i m}
\pdv{\Out i m}{\Input i m} \\
\pdv[2]{E}{{\Input j {m+1}}} &= 
\pdv[2]{E}{{\Out j {m+1}}}
\left(\sigma^{\prime}\left({\Input j {m+1}}\right)\right)^2
+
\pdv{E}{{\Out j {m+1}}}\sigma^{\prime\prime}\left(\Input j {m+1}\right)
\end{align}