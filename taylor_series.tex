\subsection{Taylor Series Representation of Error}
Let us denote the total error from the optimally trained neural network for any given validation dataset by $E$. $E$ can be seen as a function of $O$, where $O$ is the output of any general neuron in the network. This error can be approximated at a particular neuron's output (say $O_k$) by using the 2nd order Taylor Series as,

\begin{align}
\hat E(O) \approx E(O_k) + (O-O_k)\cdot \left.\pdv{E}{O}\right|_{O_k} +  0.5\cdot (O-O_k)^2\cdot \left.\pdv[2]{E}{O}\right|_{O_k}\label{eq:ts1},
\end{align}


When a neuron is pruned, its output $O$ becomes 0. From equation \ref{eq:ts1}, the contribution $E(0)$ of this neuron, then becomes:

\begin{align}
\hat E(0) \approx E(O_k) - O_k\cdot \left.\pdv{E}{O}\right|_{O_k} +  0.5\cdot O_k^2\cdot \left.\pdv[2]{E}{O}\right|_{O_k}\label{eq:ts2}
\end{align}

Replacing $O$ by $O_k$ in equation \ref{eq:ts1} shows us that the error is approximated perfectly by equation \ref{eq:ts1} at $O_k$. Using this and equation \ref{eq:ts2} we get:

\begin{align}
\Delta E_k = \hat E(0) - \hat E(O_k)= - O_k\cdot \left.\pdv{E}{O}\right|_{O_k} + 0.5\cdot O_k^2\cdot \left.\pdv[2]{E}{O}\right|_{O_k}\label{eq:ts3},
\end{align}

where $\Delta E_k$ is the change in the total error of the network when exactly one neuron ($k$) is turned off.

